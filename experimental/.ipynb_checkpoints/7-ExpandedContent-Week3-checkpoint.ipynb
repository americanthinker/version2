{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ebd912-6b31-475e-86d9-4fac57cfcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d6c6cc-86a5-46ac-ad15-ddfe21331a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a9ba7a-2fde-4625-a963-c37335270d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.pipeline import chunk_data, create_vectors, create_parent_chunks, join_docs, groupby_episode\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9ad5e7-b0a6-4b64-8e13-f5f40bed1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../answer_key/data/huberman_minilm-256.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7a37da-0a33-4d5e-9d96-fc4f1ee2fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (23905, 13)\n",
      "Memory Usage: 2.37+ MB\n"
     ]
    }
   ],
   "source": [
    "data = FileIO.load_parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04155ec-20cf-4507-8683-69bf8b8399cd",
   "metadata": {},
   "source": [
    "### Set Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6760c576-8e12-4759-a5cd-981a168cb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 256\n",
    "#tokenizer\n",
    "encoding = tiktoken.get_encoding(encoding_name='cl100k_base')\n",
    "#text_splitter\n",
    "splitter = SentenceSplitter(chunk_overlap=0, chunk_size=chunk_size, tokenizer=encoding.encode)\n",
    "#model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bea007-4cca-40ef-af45-9911d694d4fa",
   "metadata": {},
   "source": [
    "### Get Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc58b4d-6774-46ff-acf4-06692b2a052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [d['content'] for d in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d1b3c-b6ca-4c80-9006-838b10890788",
   "metadata": {},
   "source": [
    "### Create Expanded Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "039f0afd-690f-465c-b34f-e236bdf29ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def groupby_episode(data: list[dict], key_field: str='video_id') -> list[list[dict]]:\n",
    "    '''\n",
    "    Separates entire Impact Theory corpus into individual \n",
    "    lists of discrete episodes.\n",
    "    '''\n",
    "    episodes = []\n",
    "    for _, group in groupby(data, lambda x: x[key_field]):\n",
    "        episode = [chunk for chunk in group]\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "850f8a51-36c0-4266-98c1-e5720de44dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expanded_content(data: list[dict]=None, \n",
    "                            chunk_list: list[list[str]]=None, \n",
    "                            window_size: int=1,\n",
    "                            num_episodes: int=193,\n",
    "                            key_field: str='video_id'\n",
    "                            ) -> list[list[str]]:\n",
    "    '''\n",
    "    Creates expanded content from original chunks of text, for use with \n",
    "    expanded content retrieval.  Takes in raw data in dict format or \n",
    "    accepts a list of chunked episodes already grouped. \n",
    "    \n",
    "    Window size sets the number of chunks before and after the original chunk.  \n",
    "    For example a window_size of 2 will return five joined chunks.  2 chunks \n",
    "    before original chunk, the original, and 2 chunks after the original.  \n",
    "    \n",
    "    Expanded content is grouped by podcast episode, and chunks are assumed \n",
    "    to be kept in order by which they will be joined as metadata in follow-on \n",
    "    processing.\n",
    "    '''\n",
    "    if not data and not chunk_list:\n",
    "        raise ValueError(\"Either data or a chunk_list must be passed as an arg\")\n",
    "    if data:\n",
    "        episodes = groupby_episode(data, key_field)\n",
    "        assert len(episodes) == num_episodes, f'Number of grouped episodes does not equal num_episodes ({len(episodes)} != {num_episodes})'\n",
    "        chunk_list = [[d['content'] for d in alist] for alist in episodes]\n",
    "    expanded_contents = []\n",
    "    for episode in tqdm(chunk_list):\n",
    "        episode_container = []\n",
    "        for i, chunk in enumerate(episode):\n",
    "            start = max(0, i-window_size)\n",
    "            end = i+window_size+1\n",
    "            expanded_content = ' '.join(episode[start:end])\n",
    "            episode_container.append(expanded_content)\n",
    "        expanded_contents.append(episode_container)\n",
    "    return expanded_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2aaebbd1-a7d5-424b-bb5e-6e7c9e0d9ace",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Number of grouped episodes does not equal num_episodes (193 != 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m expanded \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_expanded_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m, in \u001b[0;36mcreate_expanded_content\u001b[0;34m(data, chunk_list, window_size, num_episodes, key_field)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m     22\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m groupby_episode(data, key_field)\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(episodes) \u001b[38;5;241m==\u001b[39m num_episodes, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of grouped episodes does not equal num_episodes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(episodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m     chunk_list \u001b[38;5;241m=\u001b[39m [[d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m alist] \u001b[38;5;28;01mfor\u001b[39;00m alist \u001b[38;5;129;01min\u001b[39;00m episodes]\n\u001b[1;32m     25\u001b[0m expanded_contents \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of grouped episodes does not equal num_episodes (193 != 60)"
     ]
    }
   ],
   "source": [
    "expanded = create_expanded_content(data, num_episodes=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3aa8d2-2fbf-4896-8b0e-5e7ac9c57e88",
   "metadata": {},
   "source": [
    "### Create Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf0a5ce1-3b2a-4ccc-a977-177154ee34f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1256ab4a594947a59a50b7372afbc701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VECTORS:   0%|          | 0/193 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = create_vectors(chunks, model, 'cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683b848-142d-4ca1-9fad-cfe2f626c69b",
   "metadata": {},
   "source": [
    "### Join Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7024e993-2638-4051-9dc6-d2443168c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = join_docs(data, vectors, expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b6bbdb76-5daf-4b40-be03-02b89f84c172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23905"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76d413-028e-4a5e-a596-bfed1f731685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
