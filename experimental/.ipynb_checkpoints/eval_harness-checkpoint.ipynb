{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "00d7caf1-454d-4df6-a561-8169397fe6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d44c3272-54d4-4b6e-be25-1f8f76ac7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from evaluation.custom_eval_models import (CustomAzureOpenAI, AnswerCorrectnessMetric, \n",
    "                                            EvalResponse, TestCaseBundle)\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from src.reranker import ReRanker\n",
    "from src.llm.prompt_templates import context_block\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import load_azure_openai\n",
    "from src.llm.prompt_templates import (huberman_system_message, question_answering_prompt_series,\n",
    "                                     create_context_blocks, generate_prompt_series)\n",
    "from litellm import model_cost\n",
    "\n",
    "from loguru import logger\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "027d09dd-45d7-4174-8f59-1ec30ac4f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sortable_model_cost(model_cost_dict: dict) -> list[dict]:\n",
    "    '''\n",
    "    Converts a dict of dicts into a list of dicts of model names \n",
    "    and their metadata.  Only return models with an \n",
    "    'input_cost_per_token' key\n",
    "    '''\n",
    "    sortable = []\n",
    "    for k,v in model_cost_dict.items():\n",
    "        model_dict = {'model': k}\n",
    "        if 'input_cost_per_token' in v:\n",
    "            sortable.append({**model_dict, **v})\n",
    "    return sortable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa298cb7-c65c-4d99-b417-bfc74006fc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_sortable_model_cost(model_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda2020-5a6b-4514-86a6-880afa9cd8b4",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "131066a0-9471-4199-a683-954fd4a773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/golden_datasets/golden_256.json'\n",
    "data = FileIO().load_json(data_path)\n",
    "queries = list(data['queries'].values())\n",
    "\n",
    "#get random set of questions for eavl\n",
    "random_questions = sample(queries, k=25)\n",
    "assert len(random_questions) == len(set(random_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef34f1-e110-40ea-8789-b77859f5ea1c",
   "metadata": {},
   "source": [
    "### Set System Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "bc46f6f8-4413-40f5-934d-a7564a64b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2005630/564969707.py:1: ResourceWarning: unclosed <ssl.SSLSocket fd=137, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('10.18.0.6', 37056), raddr=('34.149.137.116', 443)>\n",
      "  client = get_weaviate_client()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "client = get_weaviate_client()\n",
    "collection_name = 'Huberman_minilm_256'\n",
    "reranker= ReRanker()\n",
    "llm = load_azure_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21e181-8d66-4351-99cd-d50bc62ea2e2",
   "metadata": {},
   "source": [
    "### Create Retrieval Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f2c45d-029a-41a9-9a38-3757fa035b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context_blocks(results: list[dict],\n",
    "                          summary_key: str='summary',\n",
    "                          guest_key: str='guest',\n",
    "                          content_key: str='content'):\n",
    "    context_series = [context_block.format(summary=res[summary_key],\n",
    "                                          guest=res[guest_key],\n",
    "                                          transcript=res[content_key]) \n",
    "                      for res in results]\n",
    "    return context_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6ba4a-899b-4676-817a-17395664ab6c",
   "metadata": {},
   "source": [
    "### Set Eval Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc5bb67-fd2c-4530-9ba1-109023fac080",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_eval_model = CustomAzureOpenAI('gpt-4')\n",
    "acm = AnswerCorrectnessMetric(model=azure_eval_model, strict=False).get_metric()\n",
    "faith = FaithfulnessMetric(model=azure_eval_model)\n",
    "# metrics = [AnswerCorrectnessMetric(model=azure_eval_model, strict=False).get_metric(), FaithfulnessMetric(threshold=0.7, model=azure_eval_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "26ce50d4-7428-4810-962e-b268bbf66cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohere': ['command-r', 'command-r-plus'],\n",
       " 'anthropic': ['claude-3-haiku-20240307',\n",
       "  'claude-3-sonnet-2024022',\n",
       "  'claude-3-opus-20240229'],\n",
       " 'openai': ['gpt-4-turbo-preview',\n",
       "  'gpt-4-0125-preview',\n",
       "  'gpt-4-1106-preview',\n",
       "  'gpt-4',\n",
       "  'gpt-4-0613',\n",
       "  'gpt-3.5-turbo',\n",
       "  'gpt-3.5-turbo-1106',\n",
       "  'gpt-3.5-turbo-0125']}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLM.valid_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7fc61-cca4-4a55-ab4d-836f634444c4",
   "metadata": {},
   "source": [
    "### Create Test Case(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4d1ffe-b3f7-4e2e-a63d-3900276a64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_evaluation(queries: list[str],\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      llm: LLM,\n",
    "                      ) -> list[dict]:\n",
    "    '''\n",
    "    LLM Evaluation harness that given a list of queries does the following:\n",
    "       1. Retrieves relevant context and reranks\n",
    "       2. Generates evaluated LLM actual output\n",
    "       3. Creates retrieval context for test case\n",
    "       4. Creates a text case on the fly\n",
    "       5. Given a metric execute metric evaluation\n",
    "       6. Returns list of metric evaluations\n",
    "    '''\n",
    "\n",
    "    eval_results = []\n",
    "    for query in tqdm(queries):\n",
    "        try:\n",
    "            result = client.hybrid_search(query, collection_name, limit=200)\n",
    "            reranked = reranker.rerank(result, query, top_k=3)\n",
    "            user_message = generate_prompt_series(query, reranked)\n",
    "            actual_output = llm.chat_completion(huberman_system_message, user_message, temperature=1.0)\n",
    "            retrieval_context = create_context_blocks(reranked)\n",
    "            test_case = LLMTestCase(input=query, actual_output=actual_output, retrieval_context=retrieval_context)\n",
    "            metric.measure(test_case)\n",
    "            # logger.info(test_case.input)\n",
    "            response = EvalResponse(metric=metric,\n",
    "                                    eval_model=metric.evaluation_model,\n",
    "                                    input=test_case.input,\n",
    "                                    actual_output=test_case.actual_output,\n",
    "                                    retrieval_context=test_case.retrieval_context,\n",
    "                                    score=metric.score,\n",
    "                                    reason=metric.reason\n",
    "                                    cost=metric.evaluation_cost,\n",
    "                                    eval_steps=metric.evaluation_steps)\n",
    "            eval_results.append(response)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ded51a28-a5d6-42a4-8545-774ecd77211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# eval_results = system_evaluation(random_questions[:5], client, collection_name, llm,faith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f19b43f-7d9d-4157-840d-915eaca40fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def aget_actual_outputs(user_messages: list[str]):\n",
    "    tasks = [llm.achat_completion(huberman_system_message, user_message, temperature=1.0) for user_message in user_messages]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "375642db-1c9e-4bfe-9823-402bdc7925f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def acreate_test_cases( queries: list[str],\n",
    "                              client: WeaviateWCS,\n",
    "                              collection_name: str,\n",
    "                              llm: LLM,\n",
    "                              ) -> list[LLMTestCase]:\n",
    "    '''\n",
    "    Creates a list of LLM Test Cases based on query retrievals. \n",
    "    '''\n",
    "    results = [client.hybrid_search(query, collection_name, limit=200) for query in tqdm(queries, 'QUERIES')]\n",
    "    reranked = [reranker.rerank(result, queries[i], top_k=3) for i, result in enumerate(tqdm(results, 'RERANKING'))]\n",
    "    user_messages = [generate_prompt_series(queries[i], rerank) for i, rerank in enumerate(reranked)]\n",
    "    actual_outputs = await aget_actual_outputs(user_messages)\n",
    "    retrieval_contexts = [create_context_blocks(rerank) for rerank in reranked]\n",
    "    test_cases = [LLMTestCase(input=input, actual_output=output, retrieval_context=context) \\\n",
    "                  for input, output, context in list(zip(queries, actual_outputs, retrieval_contexts))]\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "02563862-1e64-45f2-a980-663a6b08265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_cases(queries: list[str],\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      llm: LLM,\n",
    "                      ) -> list[LLMTestCase]:\n",
    "    '''\n",
    "    Creates a list of LLM Test Cases based on query retrievals. \n",
    "    '''\n",
    "    results = [client.hybrid_search(query, collection_name, limit=200) for query in tqdm(queries, 'QUERIES')]\n",
    "    reranked = [reranker.rerank(result, queries[i], top_k=3) for i, result in enumerate(tqdm(results, 'RERANKING'))]\n",
    "    user_messages = [generate_prompt_series(queries[i], rerank) for i, rerank in enumerate(reranked)]\n",
    "    actual_outputs = [llm.chat_completion(huberman_system_message, user_message, temperature=1.0) for\n",
    "                      user_message in tqdm(user_messages, 'LLM Calls')]\n",
    "    retrieval_contexts = [create_context_blocks(rerank) for rerank in reranked]\n",
    "    test_cases = [LLMTestCase(input=input, actual_output=output, retrieval_context=context) \\\n",
    "                  for input, output, context in list(zip(queries, actual_outputs, retrieval_contexts))]\n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "87fe121b-195c-4ff6-83d1-49694baf14c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUERIES: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28it/s]\n",
      "RERANKING: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.70it/s]\n",
      "LLM Calls: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 0 ns, total: 1.85 s\n",
      "Wall time: 5.55 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "questions = sample(queries, k=5)\n",
    "test_cases = create_test_cases(questions, client, collection_name, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "02356023-b847-4c09-bc6b-4a2094f41d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUERIES: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.35it/s]\n",
      "RERANKING: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "atest_cases = await acreate_test_cases(questions, client, collection_name, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c9f953b0-083a-4026-a1b4-75d9ea7de765",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def asingle_eval_call(test_case: LLMTestCase,\n",
    "                            model: DeepEvalBaseLLM,\n",
    "                            metric: FaithfulnessMetric | AnswerCorrectnessMetric,\n",
    "                            threshold: float=None\n",
    "                           ) -> EvalResponse:\n",
    "    if metric == FaithfulnessMetric:\n",
    "        threshold = threshold if threshold else 0.5\n",
    "        metric = FaithfulnessMetric(model=model, threshold=threshold)\n",
    "    if metric == AnswerCorrectnessMetric:\n",
    "        metric = AnswerCorrectnessMetric(model=model, strict=False).get_metric()\n",
    "    if metric == LatencyMetric:\n",
    "        metric = LatencyMetric(max_seconds=threshold)\n",
    "        metric.measure(test_case)\n",
    "    # await metric.a_measure(test_case)\n",
    "    response = EvalResponse(metric=metric,\n",
    "                            model=azure_eval_model.model,\n",
    "                            input=test_case.input,\n",
    "                            actual_output=test_case.actual_output,\n",
    "                            retrieval_context=test_case.retrieval_context,\n",
    "                            score=metric.score,\n",
    "                            reason=metric.reason)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "00eebff7-0627-4319-bbf2-61384d48ef79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deepeval.metrics.g_eval.g_eval.GEval at 0x7f9f317ce200>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AnswerCorrectnessMetric(model='gpt-4-turbo', strict=False).get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "62d4e4ab-ca04-47af-8a93-8d1a70168791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluation.custom_eval_models.LatencyMetric"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LatencyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a8a1275-2c41-405c-bf51-17646fcb68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def asystem_evaluation(\n",
    "                             test_cases: list[LLMTestCase],\n",
    "                             model: DeepEvalBaseLLM,\n",
    "                             metric: FaithfulnessMetric | AnswerCorrectnessMetric,\n",
    "                             threshold: float=None\n",
    "                            ):\n",
    "    tasks = [single_faith_eval(case, model, metric, threshold) for case in test_cases]\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3409bab4-5502-4804-a511-018f6ad2967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.custom_eval_models import LatencyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55f41085-ed36-4ff1-8caf-c11df3190beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 660 µs, sys: 0 ns, total: 660 µs\n",
      "Wall time: 603 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "responses = asyncio.run(asystem_evaluation(test_cases, 'gpt-4-turbo', LatencyMetric, threshold=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "44f9d041-d51b-40ff-b3c0-4482dbb9b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [r.metric for r in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "81a717b4-3f19-40c7-b526-bff359e5ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [r.score for r in responses2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fc38096f-e520-41ed-9f5a-73f090dd5e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9347887500708539,\n",
       " 0.8140108828118768,\n",
       " 0.8073533432458813,\n",
       " 0.8776524051199395,\n",
       " 0.7873294138077769]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e408cdbf-cad3-464d-ae80-1a4cb4b3dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons = [r.reason for r in responses2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ac4ff978-9325-4898-8394-c561ddc3d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorereasons = list(zip(scores, reasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ba4c63b7-525a-4cd2-9c3c-789e9fc37c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(data: list[tuple]):\n",
    "    for atuple in data:\n",
    "        print(f'SCORE: {atuple[0]}')\n",
    "        reason = atuple[1][:25]\n",
    "        print(f'REASON: {reason}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0733c5c8-a0c2-4a99-a57a-2a1ed5d74839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9347887500708539\n",
      "REASON: The actual output provide\n",
      "\n",
      "SCORE: 0.8140108828118768\n",
      "REASON: The actual output success\n",
      "\n",
      "SCORE: 0.8073533432458813\n",
      "REASON: The output accurately ref\n",
      "\n",
      "SCORE: 0.8776524051199395\n",
      "REASON: The actual output aligns \n",
      "\n",
      "SCORE: 0.7873294138077769\n",
      "REASON: The actual output effecti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(scorereasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "83cba6a9-359e-4c08-a269-17f0339de870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The actual output provides a factual and relevant response by specifying the website and discount available for the supplements discussed on the Huberman Lab Podcast, aligning well with the retrieval context. It addresses the information requirement directly and succinctly, although it could have elaborated slightly more on the types of supplements or specific episodes that discuss them for enhanced completeness.',\n",
       " 'The actual output successfully aligns with the retrieval context, particularly emphasizing the role of the anterior mid-singulate cortex and the engagement in challenging activities that are pivotal for maintaining cognitive functions akin to those of younger individuals. However, it could have included more detailed examples or specific protocols mentioned in the retrieval context to enhance comprehensiveness.',\n",
       " 'The output accurately reflects the information from the retrieval context regarding the impact of metabolites and lactate pathways on brain function, specifically linking to brain fog and lack of focus due to excessive probiotic intake. It effectively addresses the query by summarizing the relevant scientific insights, though it could slightly enhance the specificity by citing the studies or providing a bit more detail on the mechanisms.',\n",
       " 'The actual output aligns closely with the retrieval context, accurately summarizing and simplifying the detailed explanation of how REM sleep processes emotional events and irrelevant meanings, thus reflecting a high level of factual accuracy and relevance to the input question. However, it could have included a brief mention of the mechanisms like EMDR or ketamine therapy discussed in the context, to enhance completeness.',\n",
       " \"The actual output effectively captures the essence of the political and social landscape's reflection on not setting aside egos, aligning well with the discussion themes in the retrieval context. However, it could enhance its factual accuracy by incorporating more direct references or quotes from the retrieval context to strengthen its relevance and comprehensiveness.\"]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c51239ee-be0c-4c55-b5b6-476fcbdf8b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0214"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses2[0].metric.evaluation_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0cbf31c-40cc-4eef-922b-803df0a21505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.126"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_cost = [r.metric.evaluation_cost for r in responses]\n",
    "sum(total_cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e0fa2662-1db8-49ca-ab84-8f2e4d9b618a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Compare the actual output with the retrieval context to verify factual accuracy.',\n",
       " 'Assess if the actual output effectively addresses the specific information requirement stated in the input.',\n",
       " 'Determine the comprehensiveness of the actual output in covering all key aspects mentioned in the retrieval context.',\n",
       " 'Score the actual output based on the accuracy, relevance, and completeness of the information provided between 0 and 1.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses2[0].metric.evaluation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e1a017b7-ff50-49c4-a2ea-6dd7f766d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'answer_correctness',\n",
       " 'evaluation_params': [<LLMTestCaseParams.INPUT: 'input'>,\n",
       "  <LLMTestCaseParams.ACTUAL_OUTPUT: 'actual_output'>,\n",
       "  <LLMTestCaseParams.RETRIEVAL_CONTEXT: 'retrieval_context'>],\n",
       " 'criteria': None,\n",
       " 'model': <deepeval.models.gpt_model.GPTModel at 0x7f9f4e7ab130>,\n",
       " 'using_native_model': True,\n",
       " 'evaluation_model': 'gpt-4-turbo',\n",
       " 'evaluation_steps': ['Compare the actual output with the retrieval context to verify factual accuracy.',\n",
       "  'Assess if the actual output effectively addresses the specific information requirement stated in the input.',\n",
       "  'Determine the comprehensiveness of the actual output in covering all key aspects mentioned in the retrieval context.',\n",
       "  'Score the actual output based on the accuracy, relevance, and completeness of the information provided between 0 and 1.'],\n",
       " '_threshold': 0.5,\n",
       " 'strict_mode': False,\n",
       " 'async_mode': True,\n",
       " 'evaluation_cost': 0.02305,\n",
       " 'reason': 'The actual output provides a factual and relevant response by specifying the website and discount available for the supplements discussed on the Huberman Lab Podcast, aligning well with the retrieval context. It addresses the information requirement directly and succinctly, although it could have elaborated slightly more on the types of supplements or specific episodes that discuss them for enhanced completeness.',\n",
       " 'score': 0.9347887500708539,\n",
       " 'success': True}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(responses2[0].metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d0518b-c16a-4cf9-8fa9-2358d81978f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
