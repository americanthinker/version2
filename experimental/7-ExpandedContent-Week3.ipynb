{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ebd912-6b31-475e-86d9-4fac57cfcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d6c6cc-86a5-46ac-ad15-ddfe21331a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97a9ba7a-2fde-4625-a963-c37335270d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.pipeline import chunk_data, create_vectors, create_parent_chunks, join_docs, groupby_episode\n",
    "from src.preprocessor.preprocessing import FileIO\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b9ad5e7-b0a6-4b64-8e13-f5f40bed1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../answer_key/data/huberman_minilm-256.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a7a37da-0a33-4d5e-9d96-fc4f1ee2fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (23905, 13)\n",
      "Memory Usage: 2.37+ MB\n"
     ]
    }
   ],
   "source": [
    "data = FileIO.load_parquet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071a693-7ddd-4a39-8f71-bdad0b89b371",
   "metadata": {},
   "source": [
    "No need to create a new dataset, simply use the data that you already have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266d1b3c-b6ca-4c80-9006-838b10890788",
   "metadata": {},
   "source": [
    "### Create Expanded Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "039f0afd-690f-465c-b34f-e236bdf29ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def groupby_episode(data: list[dict], key_field: str='video_id') -> list[list[dict]]:\n",
    "    '''\n",
    "    Separates entire Impact Theory corpus into individual \n",
    "    lists of discrete episodes.\n",
    "    '''\n",
    "    episodes = []\n",
    "    for _, group in groupby(data, lambda x: x[key_field]):\n",
    "        episode = [chunk for chunk in group]\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "850f8a51-36c0-4266-98c1-e5720de44dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expanded_content(data: list[dict]=None, \n",
    "                            chunk_list: list[list[str]]=None, \n",
    "                            window_size: int=1,\n",
    "                            num_episodes: int=193,\n",
    "                            key_field: str='video_id'\n",
    "                            ) -> list[list[str]]:\n",
    "    '''\n",
    "    Creates expanded content from original chunks of text, for use with \n",
    "    expanded content retrieval.  Takes in raw data in dict format or \n",
    "    accepts a list of chunked episodes already grouped. \n",
    "    \n",
    "    Window size sets the number of chunks before and after the original chunk.  \n",
    "    For example a window_size of 2 will return five joined chunks.  2 chunks \n",
    "    before original chunk, the original, and 2 chunks after the original.  \n",
    "    \n",
    "    Expanded content is grouped by podcast episode, and chunks are assumed \n",
    "    to be kept in order by which they will be joined as metadata in follow-on \n",
    "    processing.\n",
    "    '''\n",
    "    if not data and not chunk_list:\n",
    "        raise ValueError(\"Either data or a chunk_list must be passed as an arg\")\n",
    "        \n",
    "    if data:\n",
    "        # groupby data into episodes using video_id key\n",
    "        episodes = groupby_episode(data, key_field)\n",
    "        assert len(episodes) == num_episodes, f'Number of grouped episodes does not equal num_episodes ({len(episodes)} != {num_episodes})'\n",
    "\n",
    "        # extract content field and ensure episodes maintain their grouping\n",
    "        chunk_list = [[d['content'] for d in alist] for alist in episodes]\n",
    "        \n",
    "    expanded_contents = []\n",
    "    for episode in tqdm(chunk_list):\n",
    "        episode_container = []\n",
    "        for i, chunk in enumerate(episode):\n",
    "            start = max(0, i-window_size)\n",
    "            end = i+window_size+1\n",
    "            expanded_content = ' '.join(episode[start:end])\n",
    "            episode_container.append(expanded_content)\n",
    "        expanded_contents.append(episode_container)\n",
    "    return expanded_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd792d-e257-455a-998f-e4ae9262891a",
   "metadata": {},
   "source": [
    "# Assignment 3.1 - \n",
    "***\n",
    "#### *Create Expanded Content chunks and join them to existing data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "022f2d97-56f6-4596-bb53-222c3bb020b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# START YOUR CODE HERE #\n",
    "########################\n",
    "\n",
    "expanded_content = create_expanded_content(None)\n",
    "flattened_content = None\n",
    "\n",
    "def join_expanded_content(data: list[dict],\n",
    "                          new_content: list[list[str]]\n",
    "                         ) -> list[dict]:\n",
    "    \n",
    "    return \n",
    "########################\n",
    "# END YOUR CODE HERE #\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc76d413-028e-4a5e-a596-bfed1f731685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 193/193 [00:00<00:00, 5742.08it/s]\n"
     ]
    }
   ],
   "source": [
    "test = create_expanded_content(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff8e4d56-e006-4098-939f-50b07377a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = [chunk for alist in test for chunk in alist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01231a77-cbf8-44a6-a00c-a27257880d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data):\n",
    "    d['expanded_content'] = flattened[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cc86127-1cf9-4f4b-bf59-3b83ef266cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    assert d.get('expanded_content', -1) != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b57b3-15d3-47ba-8594-f1ea482a5d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsa",
   "language": "python",
   "name": "vsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
