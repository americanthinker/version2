{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "424b4361-f20c-4f54-afff-7ba76578667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "envs = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dba2cec-e732-4a67-9e59-bfaf14d2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "\n",
    "from src.database.database_utils import get_weaviate_client\n",
    "from src.database.weaviate_interface_v4 import WeaviateWCS\n",
    "from src.llm.llm_interface import LLM\n",
    "from src.llm.llm_utils import get_token_count\n",
    "from src.llm.prompt_templates import question_answering_prompt_series, huberman_system_prompt\n",
    "from app_features import generate_prompt_series\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from litellm import ModelResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f89c8ad-7fb8-4982-accd-dba8436fc84e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 1, strict: True, evaluation model: gpt-4, reason: The score is 1.00 because the response perfectly addressed the concern raised in the input without any irrelevant statements., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra costs.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra costs.']\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x7f40b86ea920>], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra costs.', expected_output=None, context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra costs.'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7, model='gpt-4', strict_mode=True)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n",
    "    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n",
    ")\n",
    "evaluate([test_case], [answer_relevancy_metric], run_async=False, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d4ebecf-5553-4707-8ee8-1e35fc1fc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Give a brief explanation of how brain neuroplasticity works\",\n",
    "             \"What is the role of dopamine in the body\",\n",
    "             \"What is a catecholimine\",\n",
    "             \"What does Jocko have to say about leadership\",\n",
    "             \"What does Fridman think about the evolution of AI\", \n",
    "             \"Who is the host of the Huberman Labs podcast\",\n",
    "             \"Why do people make self-destructive decisions\",\n",
    "             \"Provide better sleep protocol in list format\",\n",
    "             \"What are the topcis that Lex Fridman discusses\",\n",
    "             \"Is there a generally positive outlook on the future of AI\",\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "444bf30f-9a5b-4994-8a77-3c0ef571e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_weaviate_client()\n",
    "turbo = LLM(model_name='gpt-3.5-turbo-0125')\n",
    "collection_name = 'Huberman_minilm_512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7aca859-f202-48e9-b329-e39ca6505db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_bundle(query: str,\n",
    "                      client: WeaviateWCS,\n",
    "                      collection_name: str,\n",
    "                      answer_llm: LLM,\n",
    "                      ground_truth_llm: LLM=None\n",
    "                     ) -> tuple[str, list[list[str]], str]:\n",
    "    '''\n",
    "    Returns answer, ground truth and associated context from a single query.\n",
    "    '''\n",
    "    def format_llm_response(response: ModelResponse) -> str:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    #1st-stage retrieval (get contexts)\n",
    "    context = client.hybrid_search(query, collection_name, \n",
    "                                   query_properties=['content', 'title', 'short_description'],\n",
    "                                   limit=3, \n",
    "                                   return_properties=['content', 'guest', 'short_description'])\n",
    "    #create contexts from content field\n",
    "    contexts = [d['content'] for d in context]\n",
    "    \n",
    "    #generate assistant message prompt\n",
    "    assist_message = generate_prompt_series(query, context, summary_key='short_description')\n",
    "\n",
    "    #generate answers from model being evaluated\n",
    "    answer = format_llm_response(answer_llm.chat_completion(huberman_system_prompt, assist_message))\n",
    "\n",
    "    #create ground truth answers\n",
    "    if ground_truth_llm:\n",
    "        ground_truth = format_llm_response(ground_truth_llm.chat_completion(huberman_system_prompt, assist_message))\n",
    "        return query, contexts, answer, ground_truth\n",
    "    return query, contexts, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6a84919-1f2d-43e9-9527-aa3b81ae07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from time import sleep\n",
    "\n",
    "def create_test_dataset(questions: list[str], \n",
    "                          client: WeaviateWCS,\n",
    "                          collection_name: str,\n",
    "                          answer_llm: LLM,\n",
    "                          ground_truth_llm: LLM=None, \n",
    "                          batch_size: int=5, \n",
    "                          disable_internal_tqdm: bool=False):\n",
    "    total = len(questions)\n",
    "    progress = tqdm('Queries', total=total, disable=disable_internal_tqdm)\n",
    "    data = []\n",
    "    batches = ceil(total/batch_size)\n",
    "    for i in range(batches):\n",
    "        batch = questions[i*batch_size:(i+1)*batch_size]\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count() * 2) as executor:\n",
    "            futures = [executor.submit(get_answer_bundle, query, client, collection_name, answer_llm) for query in batch]\n",
    "            for future in as_completed(futures):\n",
    "                progress.update(1)\n",
    "                data.append(future.result())\n",
    "        print(f\"Finished with batch {i+1}, taking a break...\")\n",
    "    queries = [d[0] for d in data]\n",
    "    contexts = [d[1] for d in data]\n",
    "    answers = [d[2] for d in data]\n",
    "    if len(data) == 4:\n",
    "        ground_truths = [d[3] for d in data]\n",
    "        return queries, contexts, answers, ground_truths\n",
    "    return queries, context, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abd639-e47b-44bc-971f-8602ab30f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                            | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "data = create_test_dataset(questions, client, collection_name, turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48edd30-c6b9-40be-80cd-cf43c25f2c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
